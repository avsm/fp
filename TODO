TODO:

- value () attributes need to handle assignments for multiple cases
  for possibly multiple branches, with support for both nested and
  serial branching.

  Example A:

  len   : int  value (|f=n1,f.g=m1 -> sizeof(f.g.fi1)
                      |f=n1,f.g=m2 -> sizeof(f.g.fi2)-1
                      |f=n2        -> e
                      | _          -> e')

  case1 : int  value (|f=n1,f.g=m1 -> (e1)
                      |f=n1,f.g=m2 -> (e2)
                      |f=n2 -> (e3))
  f     : classify (case1) {
          |eo1:n1 -> {
                        g : classify (case2) {
                            |ei1:m1 -> fi1 : int[len]
                            |ei2:m2 -> fi2 : int[len+1]
                        }
          |eo2:n2 -> {f2}
  }

  Example B:

  case1 : int  value(|f=n1 -> ef1 |f=n2 -> ef2)
  case2 : int  value(|g=m1 -> es1 |g=m2 -> es2)
  len1  : int  value(|f=n1 -> sizeof(f.g) |g=m2 -> sizeof(g.l))
  len2  : int  value(|f=n2 -> sizeof(f.h) |g=m1 -> sizeof(g.k))

  f      : classify (case1) {
           |ef1:n1 -> g : int[len1];
           |ef2:n2 -> h : int[len2];
  }

  g      : classify (case2) {
           |es1:m1 -> k : int[len2];
           |es2:m2 -> l : int[len1];
  }

  Algorithm modifications:

  . After the parsing stage is typechecked for a format, construct a
    complete branching tree for it.

  . When checking the attributes, ensure that the cases specified for
    a value attribute are contained in the branching tree.  If no
    cases are specified, it is equivalent to the use of a default
    ("_") case.  Note that the variables in a value expression will
    have to be converted to the appropriate paths for each case.

  . The dependency checking will be performed for each leaf of the
    branching tree.

    Note that many leaves will share the same dependency graph, and so
    the same graph will be checked multiple times.  This can be
    optimized later.

    Another thing that can be done later is checking whether the value
    attribute of a field has a case specification corresponding to
    each use of the field.

- Lib: Handcode pcap -> ethernet -> ipv4 -> tcp using fp_lib.

- Ott: Handle nested structs.  We need to specify an appropriate
  environment namespace mechanism to handle variable lookups for
  nested structs.

- Ott:  type checking the format generation.

  Taking a step back, it is useful to keep in mind that each format
  specification has two facets: a parsing facet, and a generation
  facet.

  There are various stages to checking a format specification:

  . type checking the parsing facet

  . type checking the generation facet

    We do both these stages together.

  . sanity checking the generation facet

    This ensures that formats satisfying the value attributes of the
    fields in the specification can actually be generated, given
    appropriate inputs.  This primarily involves checking for circular
    dependencies in the value expressions.

    The scheme to do this uses a fairly obvious dependency graph
    approach, but is noted here for completeness.  We treat each value
    expression as a single-level directed tree (i.e. consisting of
    only a root node and leaf nodes, with edges being directed from
    the root to the leaves).  The leaf nodes in this tree are of two
    types: the first type are the variables in the expression that are
    references to the other fields of the format.  (A field cannot
    refer to itself in its value expression for obvious reasons.)  We
    also introduce shadow variables for each array or vector valued
    field which denote the length of that array or vector.  Value
    expressions that have a sizeof(field) subexpression will cause the
    shadow variable for field to be added as a leaf node to the tree.

    Note:

    . shadow variables do not appear in the specification, and hence
      do not have value expressions, and cannot be the root nodes of
      any expression trees.

    . fields with array or vector types cannot have value expressions
      (this is ensured by the type checker, and so there's no need to
      enforce this at a syntactic level.

    These expression trees are then stitched together by attaching the
    root node of a field's expression tree to other trees where that
    field appears as a leaf node.  If the resulting graph has any
    cycles, then we immediately have an unconstructible format.  (A
    field refering to itself in its value expression is a special case
    of this.)

  . checking that the parsing facet and the generation facet describe
    the same format

    This step is limited in what it can do.  To guarantee that the two
    facets match, we need to either rely on programmers not making
    mistakes, or severely limit the expressible formats.  The latter
    is not advisable, since it can actually be useful to allow the
    generation of formats that do not follow the parsing
    specification.  Generating invalid formats can be to test the
    error handling of the parsing code.

    So this checking stage is used to autocompute as many fields of
    the format for the generation facet in a manner consistent with
    the parsing facet of the specification; if we cannot do this, we
    merely warn about possible inconsistencies, and fall back to
    requiring the programmer to explicitly provide the field values.

    Fields for which auto-computation is possible are:

    - fields used for classification branches, provided the branch
      expressions are constants

    - fields that occur in vector length expressions, provided the
      length expression is a single variable

    - fields used in array length expressions, provided the array
      length expression is a single variable

    For example, we would like to check is that in cases of

    format vec { len : int; arr = byte[len] }

    the value for the len field is autocomputed by the generation
    function for vec from its argument value for the arr value.  This
    would statically ensure that we don't generate a format where the
    len value does not match the length of the arr vector.  Needless
    to say, we cannot autocompute array or vector valued fields.

    To implement the checking described in this section, the
    dependency graph described above is extended by ensuring that the
    shadow variable of a array/vector typed field (e.g. arr) is a
    direct child of fields (e.g. len) that appear in its array length
    expression; this dependency is marked using a special edge.  This
    ensures that the programmer has specified a value expression
    taking into account the parsing-generation linkage, even though we
    cannot verify its correctness.  If there is no such direct child
    relationship, then we emit a warning, and do not emit autocompute
    code for the field (i.e. len).

    Here's an example of a format that would pass this check, but
    violates parsing-generation correctness:

    format { len: int value(sizeof(arr1) + sizeof(arr2));
             arr1: int[len];
             arr2: byte[len+2]; }

    The programmer can generate messages that would cause the
    corresponding parser to parse garbage.

  A note on scoping for nested formats: This differs for the parsing
  and the generation facets.  The reason for this is that during
  parsing, the formats are constructed from outside in.  Hence, any
  expressions used during parsing (e.g vector and array length
  expressions) can contain free variables referring to the embedding
  format.  However, the generation facet constructs formats from
  inside out: a nested format does not know where it will be embedded,
  and hence any expressions used during generation (e.g. value
  expressions) cannot (i.e. _should_ not: this needs to be enforced
  during typechecking) contain free variables referring to embedding
  formats.  In other words, this is an implementation of the layering
  principle, and having enclosed formats referring to enclosing
  formats is a layering violation.

- Typechecking output:

  This will be essentially a cooked version of the ast:

  . Variable names will be resolved into identifiers, so that lexical
    scoping is resolved.  Note that variables in expressions are
    always references, not definitions.  These references should be
    represented in a manner that makes it easy to determine the free
    variables at arbitrary scope (e.g. using an integer to indicate
    the nesting levels between the variable definition and its
    reference)

  . Expressions will be constant folded.

  . Expressions in variant definitions will be entered into the
    environment in their AST form, since they cannot be checked or
    constant folded without knowing their required type.  This is only
    known when they are associated with a field; so the variant
    definition is extracted from the environment when processing such
    a field, and is then type checked and constant-folded in the
    context of the field that uses it.

    Expressions in every other context (within a format definition)
    can have a type deduced for it; these contexts are (i) as
    arguments to a function, (ii) as values of value attributes, and
    (iii) as values of inline variant definitions.

  . Each module will be annotated with the set of identifiers denoting
    free variables appearing in (possibly nested) value expressions
    within its body.

- Environment design for the typechecker:

  . This should be designed to as closely mimic the semantics as
    efficiently as possible; which implies that it should be a stack
    of mappings, where each mapping can be represented as a list, and
    each list entry is a cons of an identifier and its env info.

Notes:

- A bytestream view is implemented as an abstract datatype in its own
  module.

- Each datatype is considered to be an interpretation of a view of a
  bytestream.

  To implement this, the representation types for the datatype modules
  are the view, not the evaluation type.  The evaluation type needs to
  also be exported in the module signature, to implement array
  functors (see below).

- The array datatype is implemented as a functor, that takes as an
  argument the module for a primitive datatype.  Note that arrays of
  arrays will probably not trivially work with this approach.

- The representation type of a dependent sum will be an object of a
  class whose constructor will take as arguments the representation
  types of each label.

- Lib: We handle errors that arise when unmarshalling beyond the limit
  of the buffer by throwing an appropriate exception.  This way, the
  user can retry with a bigger buffer.

- We use a Map kind for case fields.
