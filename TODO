TODO:

- Test Types.{ident_map, free_variables}.

- Add a check to ensure that array structs don't have free variables,
  to avoid having to deal with value extraction from within arrays.
  This is a generation-only check.

- Lib: Handcode pcap -> ethernet -> ipv4 -> tcp using fp_lib.

- Ott:  type checking the format generation.

  Taking a step back, it is useful to keep in mind that each format
  specification has two facets: a parsing facet, and a generation
  facet.

  There are various stages to checking a format specification:

  . type checking the parsing facet

  . type checking the generation facet

    We do both these stages together.

  . sanity checking the generation facet

    This ensures that formats satisfying the value attributes of the
    fields in the specification can actually be generated, given
    appropriate inputs.  This primarily involves checking for circular
    dependencies in the value expressions, or redundant value
    expressions.

    Redundant value expressions result when the user provides value
    expressions for fields (say, f) that are the focus of classify
    expressions (say, for field g).  That is, we have,

      f : byte;
      g : classify (f) {
          |1:case1 -> ...
          |2..5:case2 -> ...
          ...
          }
      }

    Such fields (g) have an implicit value expression that computes
    the value based on the case value of f.  In the example above, f
    has an implicit branch case value (|g=case1 -> 1).  This could
    conflict with what the user provides, so we need to check such
    value expressions, which is simple to check for.  In case the
    branch is a range (e.g. case2 above), then the field must specify
    a value attribute (since we cannot compute a unique value for f in
    this case, the user needs to specify one).  If a value attribute
    is specified, it should be consistent (i.e. the values should
    match in the const cases, and be within range in the non-const
    cases.)

    We now discuss circular dependencies.

    The scheme to do this uses a fairly obvious dependency graph
    approach, but is noted here for completeness.  We treat each value
    expression as a single-level directed tree (i.e. consisting of
    only a root node and leaf nodes, with edges being directed from
    the root to the leaves).  The leaf nodes in this tree are of two
    types: the first type are the variables in the expression that are
    references to the other fields of the format.  (A field cannot
    refer to itself in its value expression for obvious reasons.)  We
    also introduce shadow variables for each array or vector valued
    field which denote the length of that array or vector.  Value
    expressions that have a sizeof(field) subexpression will cause the
    shadow variable for field to be added as a leaf node to the tree.

    Note:

    . shadow variables do not appear in the specification, and hence
      do not have value expressions, and cannot be the root nodes of
      any expression trees.

    . fields with array or vector types cannot have value expressions
      (this is ensured by the type checker, and so there's no need to
      enforce this at a syntactic level.

    These expression trees are then stitched together by attaching the
    root node of a field's expression tree to other trees where that
    field appears as a leaf node.  If the resulting graph has any
    cycles, then we immediately have an unconstructible format.  (A
    field refering to itself in its value expression is a special case
    of this.)

  . checking that the parsing facet and the generation facet describe
    the same format

    This step is limited in what it can do.  To guarantee that the two
    facets match, we need to either rely on programmers not making
    mistakes, or severely limit the expressible formats.  The latter
    is not advisable, since it can actually be useful to allow the
    generation of formats that do not follow the parsing
    specification.  Generating invalid formats can be to test the
    error handling of the parsing code.

    So this checking stage is used to autocompute as many fields of
    the format for the generation facet in a manner consistent with
    the parsing facet of the specification; if we cannot do this, we
    merely warn about possible inconsistencies, and fall back to
    requiring the programmer to explicitly provide the field values.

    Fields for which auto-computation is possible are:

    - fields used for classification branches, provided the branch
      expressions are constants

    - fields that occur in vector length expressions, provided the
      length expression is a single variable

    - fields used in array length expressions, provided the array
      length expression is a single variable

    For example, we would like to check is that in cases of

    format vec { len : int; arr = byte[len] }

    the value for the len field is autocomputed by the generation
    function for vec from its argument value for the arr value.  This
    would statically ensure that we don't generate a format where the
    len value does not match the length of the arr vector.  Needless
    to say, we cannot autocompute array or vector valued fields.

    To implement the checking described in this section, the
    dependency graph described above is extended by ensuring that the
    shadow variable of a array/vector typed field (e.g. arr) is a
    direct child of fields (e.g. len) that appear in its array length
    expression; this dependency is marked using a special edge.  This
    ensures that the programmer has specified a value expression
    taking into account the parsing-generation linkage, even though we
    cannot verify its correctness.  If there is no such direct child
    relationship, then we emit a warning, and do not emit autocompute
    code for the field (i.e. len).

    Here's an example of a format that would pass this check, but
    violates parsing-generation correctness:

    format { len: int value(sizeof(arr1) + sizeof(arr2));
             arr1: int[len];
             arr2: byte[len+2]; }

    The programmer can generate messages that would cause the
    corresponding parser to parse garbage.

  A note on scoping for nested formats: This differs for the parsing
  and the generation facets.  The reason for this is that during
  parsing, the formats are constructed from outside in.  Hence, any
  expressions used during parsing (e.g vector and array length
  expressions) can contain free variables referring to the embedding
  format.  However, the generation facet constructs formats from
  inside out: a nested format does not know where it will be embedded,
  and hence any expressions used during generation (e.g. value
  expressions) cannot (i.e. _should_ not: this needs to be enforced
  during typechecking) contain free variables referring to embedding
  formats.  In other words, this is an implementation of the layering
  principle, and having enclosed formats referring to enclosing
  formats is a layering violation.

Notes:

- A bytestream view is implemented as an abstract datatype in its own
  module.

- Each datatype is considered to be an interpretation of a view of a
  bytestream.

  To implement this, the representation types for the datatype modules
  are the view, not the evaluation type.  The evaluation type needs to
  also be exported in the module signature, to implement array
  functors (see below).

- The array datatype is implemented as a functor, that takes as an
  argument the module for a primitive datatype.  Note that arrays of
  arrays will probably not trivially work with this approach.

- The representation type of a dependent sum will be an object of a
  class whose constructor will take as arguments the representation
  types of each label.

- Lib: We handle errors that arise when unmarshalling beyond the limit
  of the buffer by throwing an appropriate exception.  This way, the
  user can retry with a bigger buffer.

- We use a Map kind for classification fields.

- Typechecking output:

  This is essentially a cooked version of the ast:

  . Variable names are resolved into identifiers, so that lexical
    scoping is resolved.  Note that variables in expressions are
    always references, not definitions.

  . Expressions are be constant folded.

  . Expressions in variant definitions are be entered into the
    environment in their AST form, since they cannot be checked or
    constant folded without knowing their required type.  The type is
    only known when they are associated with a field; so the variant
    definition is extracted from the environment when processing such
    a field, and is then type checked and constant-folded in the
    context of the field that uses it.

    Expressions in every other context (within a format definition)
    have a type deduced for it; these contexts are (i) as arguments to
    a function, (ii) as values in value attributes, and (iii) as
    values of inline variant definitions.


- value () attributes can  handle assignments for multiple cases
  for possibly multiple branches, with support for both nested and
  serial branching.

  Example A:

  len   : int  value (|f=n1,f.g=m1 -> sizeof(f.g.fi1)
                      |f=n1,f.g=m2 -> sizeof(f.g.fi2)-1
                      |f=n2        -> e
                      | _          -> e')

  case1 : int  value (|f=n1,f.g=m1 -> (e1)
                      |f=n1,f.g=m2 -> (e2)
                      |f=n2 -> (e3))
  f     : classify (case1) {
          |eo1:n1 -> {
                        g : classify (case2) {
                            |ei1:m1 -> fi1 : int[len]
                            |ei2:m2 -> fi2 : int[len+1]
                        }
          |eo2:n2 -> {f2}
  }

  Example B:

  case1 : int  value(|f=n1 -> ef1 |f=n2 -> ef2)
  case2 : int  value(|g=m1 -> es1 |g=m2 -> es2)
  len1  : int  value(|f=n1 -> sizeof(f.g) |g=m2 -> sizeof(g.l))
  len2  : int  value(|f=n2 -> sizeof(f.h) |g=m1 -> sizeof(g.k))

  f  : classify (case1) {
       |ef1:n1 -> g : int[len1];
       |ef2:n2 -> h : int[len2];
  }

  g  : classify (case2) {
       |es1:m1 -> k : int[len2];
       |es2:m2 -> l : int[len1];
  }


  . When checking the attributes, we ensure that the cases specified
    for a value attribute are contained in the branching tree.  If no
    cases are specified, it is equivalent to the use of a default
    ("_") case.

  . We also check to ensure that the case specification is complete
    (i.e. all branches are covered), and do not contain unmatched
    cases.

- We check for overlaps in classification branches.  This is a
  generation-only check.

  The algorithm for this is essentially derived from the one specified
  in Luc Maranget's "Warnings" paper.  The way we adapt it for our
  purposes is to consider each classified struct as a variant datatype
  with a tuple argument.  The root struct can be considered as an
  anonymous tuple.  An example explains:

  struct {
     a : int
     b : int value ()
     c : classify (b) {
           0:A -> {
             f : int
             g : classify (f) {
                    0:F -> { }
                    1:G -> { }
                 }
             h : byte
           }
           1:B -> {
             m : int
             n : classify (m) {
                    0:M -> { }
                    1:M -> { }
                 }
             o : int
           }
         }
     d : int
     k : classify (d) {
           0: D -> { }
           1: E -> { }
         }
  }

  This is equivalent to the following variant datatypes for pattern
  matching purposes:

  root struct = (c * k)
  and c = A of g | B of n
  and k = D | E
  and g = F | G
  and n = M | N

- We add a generation-only check to ensure that a field is not the
  target of classification multiple times, since this complicates
  generation checking:

  g : classify (f) {
       0:A -> { }
      }
  h : classify (f) {
       1:B -> { }
      }

  Then

     generate ('A g) ('B h) ...

  would result in conflicting values for f: 0 and 1.  Otherwise we
  would need to catch this case via code generation in the body for
  'generate'.  This in conjunction with ranges for classification
  branches would complicate generation checking.

  This check can be removed later.


