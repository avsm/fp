TODO:

- Add type checking for num_bit_sets, to ensure it only works on a
  scalar.  This will be a special-case, like the one for bits.

- Add a check to ensure that array structs don't have free variables,
  to avoid having to deal with value extraction from within arrays.
  This is a generation-only check.

- With variant definitions, it should be possible to specify the
  default at the time of the use of the variant definition.
  Currently, the default is only specified at the time of the
  declaration, which is restrictive.

- Notes on free-variables for parsing codegen:

  It's not enough to know what the free variables are, but we need to
  know how they occur in the expression(s), and how they are related
  to the field associated with that expression.

  Free-variables of a struct that are needed for the parse phase
  appear in:
  . vector and array length expressions
  . classification expressions

  Additional free-variables for a {array,map}-valued field occur in
  the field's array length or classify expression.  These
  free-variables are not needed as arguments to the generated struct
  parsing functions, but are needed to compute the length or branch of
  the field itself.

    For example, in

      format f {
        br  : byte;
        len : byte;
        s : ...  {
           g : array (br) {
                k : byte[len];
               }
        }
      }

    len is free in the struct G, and hence is an argument to its
    generated parsing function.  br is needed in the generated code
    that sets up field g itself.  These two groups of free-variables
    need to be stored seperately for field g.

    Both br and len are free in the struct S of field s, and hence
    belong to the same group in this case.

  remaining() is a special free-variable, whose name remains the same
  but whose value changes for each struct.  Its value is not derived
  from any field, but from the env/buffer passed in to the generated
  parsing function.

  Within these expressions, a free variable can appear in one or more
  of the following contexts:

  1. as a direct argument to one of the arithmetic functions.

    Currently all these functions take integer arguments.  In this
    case, the generated struct parsing function needs only the type of
    the free variable.  The integer argument for the arithmetic
    function can be computed by using the _to_int API of the module
    corresponding to the field type from the runtime library.

    The value used for the call to this generated parsing function
    will be the name of the representation of the corresponding field.

    A special case of this is when a variable directly appears as the
    length expression of a vector or array, e.g. the free-variable
    "len" in
         vec: byte[len]
         arr: array(len) {...}

  2. as a direct argument to the "length" or "array_length" function.

    The generated struct parsing function only needs the type of the
    free variable.  The length value can then be computed by a call to
    the appropriate vector/array length function from the runtime
    library.

    The value used for the call to this generated parsing function
    will be the name of the representation of the corresponding field.

  3. as a direct argument to the "offset" function.

    This case differs from the above two, since the generated struct
    parsing function will need the value of the offset, since it
    cannot determine the offset value from either or both of the
    variable name/type using (current incarnation of) the runtime
    library.

    The variable used for the call to this parsing function will have
    to be generated and its name recorded in the codegen state.  (We
    could always generate and record this for each field, irrespective
    of whether it was used as an offset, but this would not be very
    elegant.)

    To compute the offset, we would need to record the variable
    holding the env of nearest enclosing struct in the codegen
    environment.  It is simpler to do this irrespective of whether or
    not the variable is needed later for offset computation.

  Note that variables falling under categories 1) and 2) are mutually
  exclusive, whereas variables falling under 3) could also fall under
  1) or 2).  Hence, the generated parsing functions for a struct could
  take multiple arguments derived from the same free field, and
  generated calls to these functions could need multiple variables to
  hold the arguments for these functions.

  remaining(): The use of this "free-variable" does not need a
  corresponding variable to be recorded in the codegen environment.
  The codegen merely generates a call to Env.remaining with the
  current env as argument.

- Test Types.ident_map.

- A nice thing to provide good support for is optional fields.  Though
  they can be manually translated as follows:

    type_option : int;
    type : classify (type_option) {
        0:No_type -> {}
        1:Type -> {
            type : variant phone_type;
        }
    }

  it would be nice to abbreviate this at the spec level and at the
  code generation level.

- Lib: Handcode pcap -> ethernet -> ipv4 -> tcp using fp_lib.

- Ott:  type checking the format generation.

  Taking a step back, it is useful to keep in mind that each format
  specification has two facets: a parsing facet, and a generation
  facet.

  There are various stages to checking a format specification:

  . type checking the parsing facet

  . type checking the generation facet

    We do both these stages together.

  . sanity checking the generation facet

    This ensures that formats satisfying the value attributes of the
    fields in the specification can actually be generated, given
    appropriate inputs.  This primarily involves checking for circular
    dependencies in the value expressions, or redundant value
    expressions.

    Redundant value expressions result when the user provides value
    expressions for fields (say, f) that are the focus of classify
    expressions (say, for field g).  That is, we have,

      f : byte;
      g : classify (f) {
          |1:case1 -> ...
          |2..5:case2 -> ...
          ...
          }
      }

    Such fields (g) have an implicit value expression that computes
    the value based on the case value of f.  In the example above, f
    has an implicit branch case value (|g=case1 -> 1).  This could
    conflict with what the user provides, so we need to check such
    value expressions, which is simple to check for.  In case the
    branch is a range (e.g. case2 above), then the field must specify
    a value attribute (since we cannot compute a unique value for f in
    this case, the user needs to specify one).  If a value attribute
    is specified, it should be consistent (i.e. the values should
    match in the const cases, and be within range in the non-const
    cases.)

    We now discuss circular dependencies.

    The scheme to do this uses a fairly obvious dependency graph
    approach, but is noted here for completeness.  We treat each value
    expression as a single-level directed tree (i.e. consisting of
    only a root node and leaf nodes, with edges being directed from
    the root to the leaves).  The leaf nodes in this tree are of two
    types: the first type are the variables in the expression that are
    references to the other fields of the format.  (A field cannot
    refer to itself in its value expression for obvious reasons.)  We
    also introduce shadow variables for each array or vector valued
    field which denote the length of that array or vector.  Value
    expressions that have a sizeof(field) subexpression will cause the
    shadow variable for field to be added as a leaf node to the tree.

    Note:

    . shadow variables do not appear in the specification, and hence
      do not have value expressions, and cannot be the root nodes of
      any expression trees.

    . fields with array or vector types cannot have value expressions
      (this is ensured by the type checker, and so there's no need to
      enforce this at a syntactic level.

    These expression trees are then stitched together by attaching the
    root node of a field's expression tree to other trees where that
    field appears as a leaf node.  If the resulting graph has any
    cycles, then we immediately have an unconstructible format.  (A
    field refering to itself in its value expression is a special case
    of this.)

  . checking that the parsing facet and the generation facet describe
    the same format

    This step is limited in what it can do.  To guarantee that the two
    facets match, we need to either rely on programmers not making
    mistakes, or severely limit the expressible formats.  The latter
    is not advisable, since it can actually be useful to allow the
    generation of formats that do not follow the parsing
    specification.  Generating invalid formats can be used to test the
    error handling of the parsing code.

    So this checking stage is used to autocompute as many fields of
    the format for the generation facet in a manner consistent with
    the parsing facet of the specification; if we cannot do this, we
    merely warn about possible inconsistencies, and fall back to
    requiring the programmer to explicitly provide the field values.

    Fields for which auto-computation is possible are:

    - fields used for classification branches, provided the branch
      expressions are constants

    - fields that occur in vector length expressions, provided the
      length expression is a single variable

    - fields used in array length expressions, provided the array
      length expression is a single variable

    For example, we would like to check is that in cases of

    format vec { len : int; arr = byte[len] }

    the value for the len field is autocomputed by the generation
    function for vec from its argument value for the arr value.  This
    would statically ensure that we don't generate a format where the
    len value does not match the length of the arr vector.  Needless
    to say, we cannot autocompute array or vector valued fields.

    To implement the checking described in this section, the
    dependency graph described above is extended by ensuring that the
    shadow variable of a array/vector typed field (e.g. arr) is a
    direct child of fields (e.g. len) that appear in its array length
    expression; this dependency is marked using a special edge.  This
    ensures that the programmer has specified a value expression
    taking into account the parsing-generation linkage, even though we
    cannot verify its correctness.  If there is no such direct child
    relationship, then we emit a warning, and do not emit autocompute
    code for the field (i.e. len).

    Here's an example of a format that would pass this check, but
    violates parsing-generation correctness:

    format { len: int value(sizeof(arr1) + sizeof(arr2));
             arr1: int[len];
             arr2: byte[len+2]; }

    The programmer can generate messages that would cause the
    corresponding parser to parse garbage.

  A note on scoping for nested formats: This differs for the parsing
  and the generation facets.  The reason for this is that during
  parsing, the formats are constructed from outside in.  Hence, any
  expressions used during parsing (e.g vector and array length
  expressions) can contain free variables referring to the embedding
  format.  However, the generation facet constructs formats from
  inside out: a nested format does not know where it will be embedded,
  and hence any expressions used during generation (e.g. value
  expressions) cannot (i.e. _should_ not: this needs to be enforced
  during typechecking) contain free variables referring to embedding
  formats.  In other words, this is an implementation of the layering
  principle, and having enclosed formats referring to enclosing
  formats is a layering violation.

Notes:

- A bytestream view is implemented as an abstract datatype in its own
  module.

- Each datatype is considered to be an interpretation of a view of a
  bytestream.

  To implement this, the representation types for the datatype modules
  are the view, not the evaluation type.  The evaluation type needs to
  also be exported in the module signature, to implement array
  functors (see below).

- The array datatype is implemented as a functor, that takes as an
  argument the module for a primitive datatype.  Note that arrays of
  arrays will probably not trivially work with this approach.

- The representation type of a dependent sum will be an object of a
  class whose constructor will take as arguments the representation
  types of each label.

- Lib: We handle errors that arise when unmarshalling beyond the limit
  of the buffer by throwing an appropriate exception.  This way, the
  user can retry with a bigger buffer.

- We use a Map kind for classification fields.

- Typechecking output:

  This is essentially a cooked version of the ast:

  . Variable names are resolved into identifiers, so that lexical
    scoping is resolved.  Note that variables in expressions are
    always references, not definitions.

  . Expressions are be constant folded.

  . Expressions in variant definitions are be entered into the
    environment in their AST form, since they cannot be checked or
    constant folded without knowing their required type.  The type is
    only known when they are associated with a field; so the variant
    definition is extracted from the environment when processing such
    a field, and is then type checked and constant-folded in the
    context of the field that uses it.

    Expressions in every other context (within a format definition)
    have a type deduced for it; these contexts are (i) as arguments to
    a function, (ii) as values in value attributes, and (iii) as
    values of inline variant definitions.


- value () attributes can  handle assignments for multiple cases
  for possibly multiple branches, with support for both nested and
  serial branching.

  Example A:

  len   : int  value (|f=n1,f.g=m1 -> sizeof(f.g.fi1)
                      |f=n1,f.g=m2 -> sizeof(f.g.fi2)-1
                      |f=n2        -> e
                      | _          -> e')

  case1 : int  value (|f=n1,f.g=m1 -> (e1)
                      |f=n1,f.g=m2 -> (e2)
                      |f=n2 -> (e3))
  f     : classify (case1) {
          |eo1:n1 -> {
                        g : classify (case2) {
                            |ei1:m1 -> fi1 : int[len]
                            |ei2:m2 -> fi2 : int[len+1]
                        }
          |eo2:n2 -> {f2}
  }

  Example B:

  case1 : int  value(|f=n1 -> ef1 |f=n2 -> ef2)
  case2 : int  value(|g=m1 -> es1 |g=m2 -> es2)
  len1  : int  value(|f=n1 -> sizeof(f.g) |g=m2 -> sizeof(g.l))
  len2  : int  value(|f=n2 -> sizeof(f.h) |g=m1 -> sizeof(g.k))

  f  : classify (case1) {
       |ef1:n1 -> g : int[len1];
       |ef2:n2 -> h : int[len2];
  }

  g  : classify (case2) {
       |es1:m1 -> k : int[len2];
       |es2:m2 -> l : int[len1];
  }


  . When checking the attributes, we ensure that the cases specified
    for a value attribute are contained in the branching tree.  If no
    cases are specified, it is equivalent to the use of a default
    ("_") case.

  . We also check to ensure that the case specification is complete
    (i.e. all branches are covered), and do not contain unmatched
    cases.

- We check for overlaps in classification branches.  This is a
  generation-only check.

  The algorithm for this is essentially derived from the one specified
  in Luc Maranget's "Warnings" paper.  The way we adapt it for our
  purposes is to consider each classified struct as a variant datatype
  with a tuple argument.  The root struct can be considered as an
  anonymous tuple.  An example explains:

  struct {
     a : int
     b : int value ()
     c : classify (b) {
           0:A -> {
             f : int
             g : classify (f) {
                    0:F -> { }
                    1:G -> { }
                 }
             h : byte
           }
           1:B -> {
             m : int
             n : classify (m) {
                    0:M -> { }
                    1:M -> { }
                 }
             o : int
           }
         }
     d : int
     k : classify (d) {
           0: D -> { }
           1: E -> { }
         }
  }

  This is equivalent to the following variant datatypes for pattern
  matching purposes:

  root struct = (c * k)
  and c = A of g | B of n
  and k = D | E
  and g = F | G
  and n = M | N

- We add a generation-only check to ensure that a field is not the
  target of classification multiple times, since this complicates
  generation checking:

  g : classify (f) {
       0:A -> { }
      }
  h : classify (f) {
       1:B -> { }
      }

  Then

     generate ('A g) ('B h) ...

  would result in conflicting values for f: 0 and 1.  Otherwise we
  would need to catch this case via code generation in the body for
  'generate'.  This in conjunction with ranges for classification
  branches would complicate generation checking.

  This check can be removed later via a command-line option: this
  would result in generation code that remove such branching fields
  from the auto-computed set.
